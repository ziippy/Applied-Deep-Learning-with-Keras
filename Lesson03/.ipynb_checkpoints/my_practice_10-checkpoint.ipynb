{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분류 시뮬레이션 데이터 생성\n",
    "import numpy\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=200, n_features=10, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples in the Dataset =  200\n",
      "Number of Features for each sample =  10\n",
      "Possible Output Classes =  [0 1]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 세트 크기 출력\n",
    "print(\"Number of Examples in the Dataset = \", X.shape[0])\n",
    "print(\"Number of Features for each sample = \", X.shape[1])\n",
    "print(\"Possible Output Classes = \", numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#모델을 케라스 순차모델로 선언\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델에 크기가 10 이고 활성화 함수는 tanh 인 hidden layer 를 하나 추가한다.\n",
    "# 입력 차원이 10 이다. features 값\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(10, activation='tanh', input_dim=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크기가 5 이고, 활성화 함수가 tanh 인 hidden layer 하나 더 추가한다.\n",
    "# 첫 번째가 아닌 레이어의 입력 차원은, keras 가 알아서 알기 때문에, input_dim 설정이 필요없다.\n",
    "\n",
    "model.add(Dense(5, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 활성화 함수를 사용하는 결과 레이어를 추가한다.\n",
    "# 결과 레이어의 유닛 수는 결과 차원 수와 일치해야 한다.\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compile() 메서드를 사용해 모델 훈련에 사용할 손실 함수로 binary crossentropy 를, 최적화 함수는 SGD 도 지정한다.\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ziippy/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6590\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 328us/step - loss: 0.5405\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 958us/step - loss: 0.4457\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3700\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3106\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2638\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2275\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1987\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 972us/step - loss: 0.1759\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 959us/step - loss: 0.1574\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 950us/step - loss: 0.1423\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 942us/step - loss: 0.1297\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 956us/step - loss: 0.1190\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 959us/step - loss: 0.1100\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 945us/step - loss: 0.1020\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 967us/step - loss: 0.0951\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 959us/step - loss: 0.0889\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 954us/step - loss: 0.0833\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 862us/step - loss: 0.0784\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 863us/step - loss: 0.0740\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 863us/step - loss: 0.0699\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 866us/step - loss: 0.0664\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 861us/step - loss: 0.0627\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 858us/step - loss: 0.0598\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 871us/step - loss: 0.0569\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 864us/step - loss: 0.0542\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 867us/step - loss: 0.0518\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 870us/step - loss: 0.0495\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 855us/step - loss: 0.0474\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 860us/step - loss: 0.0454\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 868us/step - loss: 0.0436\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 866us/step - loss: 0.0419\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 869us/step - loss: 0.0403\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 864us/step - loss: 0.0390\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 863us/step - loss: 0.0375\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 872us/step - loss: 0.0362\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 876us/step - loss: 0.0349\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 875us/step - loss: 0.0338\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 877us/step - loss: 0.0328\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 885us/step - loss: 0.0316\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 873us/step - loss: 0.0307\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 849us/step - loss: 0.0298\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 863us/step - loss: 0.0289\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 850us/step - loss: 0.0280\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 863us/step - loss: 0.0273\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 867us/step - loss: 0.0265\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 854us/step - loss: 0.0258\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 876us/step - loss: 0.0250\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 858us/step - loss: 0.0246\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 882us/step - loss: 0.0238\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 871us/step - loss: 0.0232\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 866us/step - loss: 0.0227\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 775us/step - loss: 0.0221\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 750us/step - loss: 0.0216\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 780us/step - loss: 0.0211\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 787us/step - loss: 0.0206\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 822us/step - loss: 0.0201\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 856us/step - loss: 0.0197\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 879us/step - loss: 0.0194\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 876us/step - loss: 0.0189\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 850us/step - loss: 0.0185\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 861us/step - loss: 0.0181\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 891us/step - loss: 0.0177\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 866us/step - loss: 0.0174\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 852us/step - loss: 0.0171\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 864us/step - loss: 0.0167\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 872us/step - loss: 0.0164\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 897us/step - loss: 0.0161\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 872us/step - loss: 0.0158\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 862us/step - loss: 0.0155\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 855us/step - loss: 0.0152\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 872us/step - loss: 0.0150\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 859us/step - loss: 0.0147\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 874us/step - loss: 0.0144\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 859us/step - loss: 0.0142\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 869us/step - loss: 0.0140\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 869us/step - loss: 0.0137\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 869us/step - loss: 0.0135\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 876us/step - loss: 0.0133\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 875us/step - loss: 0.0131\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 863us/step - loss: 0.0128\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 869us/step - loss: 0.0127\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 860us/step - loss: 0.0125\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 858us/step - loss: 0.0123\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 876us/step - loss: 0.0121\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 863us/step - loss: 0.0119\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 868us/step - loss: 0.0118\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 865us/step - loss: 0.0116\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 865us/step - loss: 0.0114\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 859us/step - loss: 0.0113\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 911us/step - loss: 0.0111\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 875us/step - loss: 0.0110\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 870us/step - loss: 0.0108\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 867us/step - loss: 0.0107\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 857us/step - loss: 0.0105\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 848us/step - loss: 0.0104\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 860us/step - loss: 0.0103\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 864us/step - loss: 0.0101\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 866us/step - loss: 0.0100\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 860us/step - loss: 0.0098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd757d1bcf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit() 메서드를 사용해, 모델 훈련 횟수를 100회, 배치 크기는 5로 설정\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련된 모델을 활용해 첫 10개(x[0:10])의 입력 데이터의 결과 클래스를 예측 해 보자\n",
    "\n",
    "y_predicted = model.predict(X[0:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability for each of the examples belonging to class 1: \n",
      "[[0.09745508]\n",
      " [0.01985723]\n",
      " [0.997759  ]\n",
      " [0.00212124]\n",
      " [0.00187969]\n",
      " [0.00203314]\n",
      " [0.9968674 ]\n",
      " [0.99648374]\n",
      " [0.99752015]\n",
      " [0.00391827]]\n",
      "Predicted class label for eahc of the examples: \n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# 예측 클래스 출력\n",
    "\n",
    "print(\"Predicted probability for each of the examples belonging to class 1: \")\n",
    "print(y_predicted)\n",
    "\n",
    "print(\"Predicted class label for eahc of the examples: \")\n",
    "print(numpy.round(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 1 1 1 0]\n",
      "Metric:  accuracy:  1.0\n",
      "Metric:  f1_score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# metric 출력\n",
    "from sklearn import metrics\n",
    "\n",
    "print(y[0:10])\n",
    "\n",
    "accuracy = metrics.accuracy_score(y[0:10], numpy.round(y_predicted))\n",
    "f1_score = metrics.f1_score(y[0:10], numpy.round(y_predicted), average='macro')\n",
    "print(\"Metric:  accuracy: \", accuracy)\n",
    "print(\"Metric:  f1_score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
